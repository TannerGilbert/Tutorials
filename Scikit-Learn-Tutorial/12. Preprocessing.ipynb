{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn Tutorial #12 - Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\"><td>\n",
    "  <a target=\"_blank\"  href=\"https://colab.research.google.com/github/TannerGilbert/Tutorials/blob/master/Scikit-Learn-Tutorial/12.%20Preprocessing.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab\n",
    "  </a>\n",
    "</td><td>\n",
    "  <a target=\"_blank\"  href=\"https://github.com/TannerGilbert/Tutorials/blob/master/Scikit-Learn-Tutorial/12.%20Preprocessing.ipynb\">\n",
    "    <img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "</td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scikit Learn Logo](http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is preprocessing important?\n",
    "\n",
    "In general, algorithms benefit from standardization of the data-set. The degree of benefit is highly dependent on the score/loss the model is optimizing because for example a model like K-Nearest-Neighbors is minimizing a loss that is highly dependent on the scale (euclidean distance) whilst something like a Decision Tree doesn't care about normalization because he is just performing splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing in Scikit Learn\n",
    "\n",
    "Scikit Learns <i>sklearn.preprocessing</i> package provides a lot of different preprocessing functions. We will take a look at standardization, normalization, encoding categorical features, imputation of missing values as well as generating polynomial features for algorithms like LinearRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        label\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'label'])\n",
    "X = np.array(iris.drop(['label'], axis=1))\n",
    "y = np.array(iris['label'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "Standardization of datasets is a really common proceature. For some algorithm and datasets even required. Algorithms like SVM with RBF kernel or Ridge and Lasso Regression assume that all features are standardized. For algorithms like Nearest Neighbors it's important that features are scaled because you compute the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.80916667 3.0575     3.7275     1.1825    ]\n",
      "[0.82036535 0.44453393 1.7439401  0.75029578]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.47393679,  1.22037928, -1.5639872 , -1.30948358],\n",
       "       [-0.13307079,  3.02001693, -1.27728011, -1.04292204],\n",
       "       [ 1.08589829,  0.09560575,  0.38562104,  0.28988568],\n",
       "       [-1.23014297,  0.77046987, -1.21993869, -1.30948358],\n",
       "       [-1.7177306 ,  0.32056046, -1.39196294, -1.30948358]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "print(scaler.mean_)\n",
    "print(scaler.scale_)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_train_scaled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.35451684, -0.57925837,  0.5576453 ,  0.02332414],\n",
       "       [-0.13307079,  1.67028869, -1.16259727, -1.17620281],\n",
       "       [ 2.30486738, -1.02916778,  1.81915651,  1.48941263],\n",
       "       [ 0.23261993, -0.35430366,  0.44296246,  0.42316645],\n",
       "       [ 1.2077952 , -0.57925837,  0.61498672,  0.28988568]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also scale our data between a give minimum and maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08823529, 0.66666667, 0.        , 0.04166667],\n",
       "       [0.41176471, 1.        , 0.0877193 , 0.125     ],\n",
       "       [0.70588235, 0.45833333, 0.59649123, 0.54166667],\n",
       "       [0.14705882, 0.58333333, 0.10526316, 0.04166667],\n",
       "       [0.02941176, 0.5       , 0.05263158, 0.04166667]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_train_minmax[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52941176, 0.33333333, 0.64912281, 0.45833333],\n",
       "       [0.41176471, 0.75      , 0.12280702, 0.08333333],\n",
       "       [1.        , 0.25      , 1.03508772, 0.91666667],\n",
       "       [0.5       , 0.375     , 0.61403509, 0.58333333],\n",
       "       [0.73529412, 0.33333333, 0.66666667, 0.54166667]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "X_test_minmax[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5974026 , 0.81818182, 0.14925373, 0.08      ],\n",
       "       [0.74025974, 1.        , 0.2238806 , 0.16      ],\n",
       "       [0.87012987, 0.70454545, 0.65671642, 0.56      ],\n",
       "       [0.62337662, 0.77272727, 0.23880597, 0.08      ],\n",
       "       [0.57142857, 0.72727273, 0.19402985, 0.08      ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    "X_train_maxabs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79220779, 0.63636364, 0.70149254, 0.48      ],\n",
       "       [0.74025974, 0.86363636, 0.25373134, 0.12      ],\n",
       "       [1.        , 0.59090909, 1.02985075, 0.92      ],\n",
       "       [0.77922078, 0.65909091, 0.67164179, 0.6       ],\n",
       "       [0.88311688, 0.63636364, 0.71641791, 0.56      ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_maxabs = max_abs_scaler.transform(X_test)\n",
    "X_test_maxabs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Normalization is the process of scaling individual samples to have unit norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77577075, 0.60712493, 0.16864581, 0.03372916],\n",
       "       [0.77381111, 0.59732787, 0.2036345 , 0.05430253],\n",
       "       [0.76945444, 0.35601624, 0.50531337, 0.16078153],\n",
       "       [0.786991  , 0.55745196, 0.26233033, 0.03279129],\n",
       "       [0.78609038, 0.57170209, 0.23225397, 0.03573138]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "normalizer = Normalizer(norm='l2') # norm specifies the norm used to normalize the data\n",
    "X_train_normalized = normalizer.fit_transform(X_train)\n",
    "X_train_normalized[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73659895, 0.33811099, 0.56754345, 0.14490471],\n",
       "       [0.8068282 , 0.53788547, 0.24063297, 0.04246464],\n",
       "       [0.70600618, 0.2383917 , 0.63265489, 0.21088496],\n",
       "       [0.73350949, 0.35452959, 0.55013212, 0.18337737],\n",
       "       [0.76467269, 0.31486523, 0.53976896, 0.15743261]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_normalized = normalizer.transform(X_test)\n",
    "X_test_normalized[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical features\n",
    "\n",
    "Sometimes our data isn't in the format we want or need to use it for Machine Learning. An example is categorical data that is represented as a string like the label column of our Iris dataset. We can use something like Scikit Learns <i>LabelEncoder</i> or <a href=\"https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\">One Hot Encoding</a> to encode our data into a useful format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
       "       'Iris-setosa'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_le = le.fit_transform(y)\n",
    "y_le[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit_transform([[1], [2], [3]]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation of missing values\n",
    "\n",
    "Many real world datasets contain missing values, often encodes as blanks, NaNs or other placeholders. To use datasets in Scikit Learn we need to fill in those missing values. A basic strategy is to just delete all rows with missing values but this comes at the cost of losing all the information contained in this row. A better way is to impute the missing values. You can impute missing values with something like the columns mean, median or most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [7., 8.],\n",
       "       [4., 5.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='mean')\n",
    "imp.fit_transform([[1, 2], [7, 8], [np.nan, np.nan]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating polynomial features\n",
    "\n",
    "Often itâ€™s useful to add complexity to the model by considering nonlinear features of the input data. A simple way of doing so is to use polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  2.,  3.,  1.,  2.,  3.,  4.,  6.,  9.],\n",
       "       [ 1.,  4.,  5.,  6., 16., 20., 24., 25., 30., 36.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = [[1,2,3], [4,5,6]]\n",
    "print(X)\n",
    "poly = PolynomialFeatures(2)\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"http://scikit-learn.org/stable/modules/preprocessing.html\">Preprocessing data (Scikit Learn Documentation)</a></li>\n",
    "    <li><a href=\"https://sebastianraschka.com/faq/docs/scale-training-test.html\">Why do we need to re-use training parameters to transform test data? (Sebastian Raschka)</a></li>\n",
    "    <li><a href=\"https://machinelearningmastery.com/how-to-prepare-data-for-machine-learning/\">How to Prepare Data For Machine Learning (machinelearningmastery)</a></li>\n",
    "    <li><a href=\"https://en.wikipedia.org/wiki/Data_pre-processing\">Data pre-processing (Wikipedia)</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "That was a quick overview of preprocessing and how to implement it in Scikit Learn. \n",
    "I hope you liked this tutorial if you did consider subscribing on my <a href=\"https://www.youtube.com/channel/UCBOKpYBjPe2kD8FSvGRhJwA\">Youtube Channel</a> or following me on Social Media. If you have any question feel free to contact me."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
